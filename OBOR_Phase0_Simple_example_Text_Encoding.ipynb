{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My personal tutorial and toolkit on how to use \"Advanced Calculator\" universally for string pattern to Y\n",
    "## installing the Dependencies and stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install Dependencies on the Environment\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install tensorflow\n",
    "!pip install torch\n",
    "!pip install torch-metal #specific for specific hardware\n",
    "!pip install torchvision\n",
    "!pip install torchaudio\n",
    "!pip install gensim\n",
    "!pip install scikit-learn\n",
    "!pip install matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the Acceleration specifically for Metal and import torch as ＯＢＯＲ \n",
    "import torch as ＯＢＯＲ\n",
    "ＯＢＯＲ.backends.mps.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/user/Documents/misc/AI/MLBasicsRetraining\n",
      "datatype of the dataset var <class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011.11.02</td>\n",
       "      <td>1.36971</td>\n",
       "      <td>1.38276</td>\n",
       "      <td>1.36357</td>\n",
       "      <td>1.37477</td>\n",
       "      <td>92669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011.11.03</td>\n",
       "      <td>1.37477</td>\n",
       "      <td>1.38541</td>\n",
       "      <td>1.36564</td>\n",
       "      <td>1.38132</td>\n",
       "      <td>97623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011.11.04</td>\n",
       "      <td>1.38132</td>\n",
       "      <td>1.38677</td>\n",
       "      <td>1.37108</td>\n",
       "      <td>1.37898</td>\n",
       "      <td>82606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011.11.07</td>\n",
       "      <td>1.38268</td>\n",
       "      <td>1.38308</td>\n",
       "      <td>1.36808</td>\n",
       "      <td>1.37746</td>\n",
       "      <td>84065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011.11.08</td>\n",
       "      <td>1.37749</td>\n",
       "      <td>1.38467</td>\n",
       "      <td>1.37243</td>\n",
       "      <td>1.38327</td>\n",
       "      <td>79486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2011.11.09</td>\n",
       "      <td>1.38327</td>\n",
       "      <td>1.38584</td>\n",
       "      <td>1.35231</td>\n",
       "      <td>1.35416</td>\n",
       "      <td>91607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2011.11.10</td>\n",
       "      <td>1.35422</td>\n",
       "      <td>1.36527</td>\n",
       "      <td>1.34835</td>\n",
       "      <td>1.36057</td>\n",
       "      <td>90295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2011.11.11</td>\n",
       "      <td>1.36056</td>\n",
       "      <td>1.37947</td>\n",
       "      <td>1.35780</td>\n",
       "      <td>1.37470</td>\n",
       "      <td>73878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2011.11.14</td>\n",
       "      <td>1.38090</td>\n",
       "      <td>1.38090</td>\n",
       "      <td>1.35910</td>\n",
       "      <td>1.36321</td>\n",
       "      <td>77676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2011.11.15</td>\n",
       "      <td>1.36322</td>\n",
       "      <td>1.36405</td>\n",
       "      <td>1.34961</td>\n",
       "      <td>1.35390</td>\n",
       "      <td>82173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date     open     high      low    close  volume\n",
       "0  2011.11.02  1.36971  1.38276  1.36357  1.37477   92669\n",
       "1  2011.11.03  1.37477  1.38541  1.36564  1.38132   97623\n",
       "2  2011.11.04  1.38132  1.38677  1.37108  1.37898   82606\n",
       "3  2011.11.07  1.38268  1.38308  1.36808  1.37746   84065\n",
       "4  2011.11.08  1.37749  1.38467  1.37243  1.38327   79486\n",
       "5  2011.11.09  1.38327  1.38584  1.35231  1.35416   91607\n",
       "6  2011.11.10  1.35422  1.36527  1.34835  1.36057   90295\n",
       "7  2011.11.11  1.36056  1.37947  1.35780  1.37470   73878\n",
       "8  2011.11.14  1.38090  1.38090  1.35910  1.36321   77676\n",
       "9  2011.11.15  1.36322  1.36405  1.34961  1.35390   82173"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For now startup_data.csv\n",
    "import pandas as pd\n",
    "import os\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Print the current working directory\n",
    "print(\"Current working directory:\", current_directory)\n",
    "# source https://www.kaggle.com/datasets/manishkc06/startup-success-prediction/data\n",
    "dataset = pd.read_csv(\"datasets/EURUSD1440.csv\")\n",
    "\n",
    "TargetColumnValue=\"volume\"\n",
    "# Read \n",
    "print(f\"datatype of the dataset var\", type(dataset))\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify the target (Y) and the source (X)\n",
    "\n",
    "Source : Bunch of variable\n",
    "\n",
    "Target : Will it succeed? with the combination? what is the pattern here? (Status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Points When Wanting to do Machine Learning\n",
    "###### sidenote : Yes I know there are a lot of things, I can't keep up with others, so instead I'm just going to assume three steps\n",
    "1. Read the data, determine the Target\n",
    "2. Convert all terget into numbers (categorization if below threshold on the unique value (for instance my maximum comfortable unique value is 5) or other stuff) and Clean the data from outliers or null\n",
    "3. Sequential NN Modeling Prioritization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Then lets preprocess the data which is step 2, Convert all target into Numbers and clean the data if there's outliers or null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model may be innacurate due to many lock-on target! Vectorization is going to be use on the target_column dataset\n",
      "10\n",
      "Type for date object\n",
      "Column 'date' has 3098 unique value(s) with total row 3098 ratio 1.0 \n",
      "Column Dropped date due to object data 1.0 not string nor int or float\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['date'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 46\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn Dropped \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m due to object data \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mratioUnique\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not string nor int or float\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (ratioUnique \u001b[38;5;241m>\u001b[39m ratioUniqueThreshold):\n\u001b[0;32m---> 46\u001b[0m     \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn Dropped \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m due to Erratic Data Row ratio \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mratioUnique\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/misc/AI/MLBasicsRetraining/.venv/lib/python3.12/site-packages/pandas/core/frame.py:5568\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5420\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[1;32m   5421\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5422\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5429\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5430\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5431\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5432\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5433\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5566\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5567\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5568\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5569\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5570\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5571\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5573\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5574\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5575\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5576\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/misc/AI/MLBasicsRetraining/.venv/lib/python3.12/site-packages/pandas/core/generic.py:4785\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4783\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   4784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4785\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4787\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   4788\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m~/Documents/misc/AI/MLBasicsRetraining/.venv/lib/python3.12/site-packages/pandas/core/generic.py:4827\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4825\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4826\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4827\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4828\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4830\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4831\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/misc/AI/MLBasicsRetraining/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:7070\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   7068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m   7069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 7070\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   7071\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[1;32m   7072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['date'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# Scan for Unique Value on each Column pandas loop\n",
    "\n",
    "#Automated Data engineering\n",
    "\n",
    "#We check for the Unique Value to approach whether we want to one_hot_encoding or mapping, lets say if its below <20 lets do Mapping for the column, but this more how comfortable you are with mapping manually and automatically\n",
    "\n",
    "import numpy as np\n",
    "#mine is 20 #but it depends on how the data and you like\n",
    "threshold=10 #Threshold on max when doing manual_traditional_mapping_column before going to toomuchformetohandle_column vector method #can be adapted if its lower than the target_column unique values\n",
    "hardlimitThreshold=20\n",
    "ratioUniqueThreshold=0.1\n",
    "manual_traditional_mapping_column = []\n",
    "toomuchformetohandle_column = []\n",
    "id_column = []\n",
    "## target_column is the final value that we wanted, this will be used later on on defining NN architecture\n",
    "target_column = TargetColumnValue # Y Axis\n",
    "\n",
    "\n",
    "#dynamic mapping and vector sum mapping switch based on the target_column\n",
    "minimumReqthresholdMap = dataset[target_column].nunique()\n",
    "if (minimumReqthresholdMap <= threshold):\n",
    "    pass\n",
    "elif (minimumReqthresholdMap > threshold and minimumReqthresholdMap <= hardlimitThreshold):\n",
    "    threshold = minimumReqthresholdMap\n",
    "    print(\"Adjusting threshold for Mapping\")\n",
    "else:\n",
    "    print(\"Training Model may be innacurate due to many lock-on target! Vectorization is going to be use on the target_column dataset\")\n",
    "    pass\n",
    "\n",
    "print(threshold)\n",
    "\n",
    "for column in dataset.columns:\n",
    "    print(f\"Type for {column}\",dataset[column].dtype)\n",
    "     # Check if the column contains non-numeric values and does not contain \"id\" or end with \"_id\"\n",
    "      # Count the number of unique non-integer values in the column\n",
    "        # if ratio unique is more than for instance 0.2 or 0.3 Drop/purge the table to optimize the data for training Accuracy\n",
    "    unique_values_count = dataset[column].nunique()\n",
    "    totalDataRow = len(dataset[column])\n",
    "    ratioUnique = unique_values_count/totalDataRow\n",
    "    print(f\"Column '{column}' has {unique_values_count} unique value(s) with total row {totalDataRow} ratio {ratioUnique} \")\n",
    "\n",
    "    if ( dataset[column].dtype == \"object\" ):\n",
    "        dataset.drop(columns=[column], inplace=True)\n",
    "        print(f\"Column Dropped {column} due to object data {ratioUnique} not string nor int or float\")\n",
    "    if (ratioUnique > ratioUniqueThreshold):\n",
    "        dataset.drop(columns=[column], inplace=True)\n",
    "        print(f\"Column Dropped {column} due to Erratic Data Row ratio {ratioUnique}\")\n",
    "    else:\n",
    "        if not np.issubdtype(dataset[column].dtype, np.number) and \"id\" not in column.lower() and not column.endswith(\"_id\"):\n",
    "            # Print the column name and the number of unique values        \n",
    "            if unique_values_count <= threshold:\n",
    "                # Eligible for manual or traditional mapping\n",
    "                manual_traditional_mapping_column.append(column)\n",
    "            else:\n",
    "                # Too many unique values, recommended for one-hot encoding\n",
    "                toomuchformetohandle_column.append(column)\n",
    "        elif \"id\" in column.lower() or column.endswith(\"_id\"):\n",
    "            # Append columns containing \"id\" or ending with \"_id\" to id_column list\n",
    "            id_column.append(column)\n",
    "\n",
    "\n",
    "# So we have two category data to preprocess Narrow Unique Val and Wide Unique Val\n",
    "print(\"======================================================\")\n",
    "# For Narrow Unique Val we going to preprocess it with simple Mapping category with integer from 0 to max unique \n",
    "# for Wide Unique Val it will have a different handling for preprocessing before feeding into NN\n",
    "print(f\"Narrow Unique Val\", manual_traditional_mapping_column)\n",
    "print(f\"Wide Unique Val\", toomuchformetohandle_column)\n",
    "print(f\"ID Column\", id_column)\n",
    "print(f\"Target\", target_column)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The start of Data Preprocessing before feeding into the machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When preparing string data from a pandas DataFrame to feed into a neural network (NN), you typically need to encode the string data into a numerical format, as NNs work with numerical data. Here are some common methods to encode string data for neural network feeding:\n",
    "## id_column\n",
    "0. Just filter out non integer or float\n",
    "\n",
    "## manual_traditional_mapping_column\n",
    "1. **Label Encoding**:\n",
    "   - Assigns a unique integer to each category in a categorical variable.\n",
    "   - Often used for ordinal categorical variables where there is a clear order.\n",
    "   - Implemented using `LabelEncoder` from scikit-learn or built-in pandas functions.\n",
    "\n",
    "2. **One-Hot Encoding**:\n",
    "   - Converts categorical variables into binary vectors.\n",
    "   - Each category is represented by a binary vector with a 1 in the position corresponding to the category and 0s elsewhere.\n",
    "   - Used when there is no ordinal relationship among categories.\n",
    "   - Implemented using `OneHotEncoder` from scikit-learn or `get_dummies()` function in pandas.\n",
    "\n",
    "## toomuchformetohandle_column\n",
    "\n",
    "3. **Embedding**:\n",
    "   - Used for handling high-cardinality/too many category categorical variables.\n",
    "   - Maps each category to a lower-dimensional dense vector space.\n",
    "   - Commonly used in natural language processing (NLP) tasks.\n",
    "   - Implemented using embedding layers in deep learning frameworks like TensorFlow or PyTorch.\n",
    "\n",
    "4. **Frequency Encoding**:\n",
    "   - Encodes categories with their frequency of occurrence in the dataset.\n",
    "   - Useful when categories' frequencies correlate with the target variable.\n",
    "   - Implemented manually by calculating frequencies and mapping them to categories.\n",
    "\n",
    "5. **Target Encoding / Mean Encoding**:\n",
    "   - Replaces categorical values with the mean of the target variable for each category.\n",
    "   - Used for classification tasks when there's a strong correlation between the categorical variable and the target.\n",
    "   - Helps capture information about the target variable.\n",
    "   - Implemented manually or using libraries like category_encoders in Python.\n",
    "\n",
    "6. **Hashing Encoding**:\n",
    "   - Converts categorical variables into a fixed-length representation using hashing functions.\n",
    "   - Reduces memory usage compared to one-hot encoding, especially for high-cardinality variables.\n",
    "   - Loss of interpretability but can be efficient for large datasets.\n",
    "   - Implemented using `HashingVectorizer` from scikit-learn or built-in functions in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Looking from the manual_traditional_mapping_column do an automatic mapping value assignment to the manual manual_traditional_mapping_column \n",
    "# Initializing the modified value by starting from pandas dataFrame loaded default dataset\n",
    "preprocessedDataTrain = dataset\n",
    "\n",
    "#remove row that contains NaN or missing data\n",
    "print(\"removing null/NA and duplicates data\")\n",
    "preprocessedDataTrain.dropna(inplace=True)\n",
    "preprocessedDataTrain.drop_duplicates(inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling id_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for id only allow number on it float64 or integer64\n",
    "import re #regular expressions time\n",
    "\n",
    "def clean_numeric(value):\n",
    "    # Use regular expression to extract numeric parts\n",
    "    numeric_parts = re.findall(r'\\d+\\.*\\d*', str(value))\n",
    "    \n",
    "    # Join numeric parts and convert to float\n",
    "    cleaned_value = ''.join(numeric_parts)\n",
    "    \n",
    "    return float(cleaned_value) if cleaned_value else None\n",
    "\n",
    "\n",
    "for column in id_column:\n",
    "    print(preprocessedDataTrain[column].apply(clean_numeric))\n",
    "    preprocessedDataTrain[column] = preprocessedDataTrain[column].apply(clean_numeric) #.apply seems to apply the whole row with the filter of the function, Sweet!\n",
    "    \n",
    "    \n",
    "\n",
    "preprocessedDataTrain.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling manual_traditional_mapping_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Then scan for every column that match with the listed manual_traditional_mapping_column array or list idk python, what i care is this runs well :D\n",
    "\n",
    "\n",
    "#1st handle the easiest mapping manual_traditional_mapping_column\n",
    "\n",
    "# for each manual_traditional_mapping_column variable scan the preprocessedDataTrain variable (in context the datatype is pandas dataframe) for each column replace the value of the whole column with the count (from 0 to maximum unique value) of self-assigned unique integer for unique value\n",
    "\n",
    "for column in manual_traditional_mapping_column:\n",
    "    # Get unique values in the column\n",
    "    unique_values = preprocessedDataTrain[column].unique()\n",
    "    \n",
    "    # Create a dictionary to store unique values as keys and their counts as values\n",
    "    value_count = {val: idx for idx, val in enumerate(unique_values)}\n",
    "    \n",
    "    # Replace values in the column with their counts\n",
    "    preprocessedDataTrain[column] = preprocessedDataTrain[column].map(value_count)\n",
    "\n",
    "preprocessedDataTrain.head(5)\n",
    "## BRUH THERE\"S BUILT IN FUNCTION FOR THIS, WHY DO I HAVE TO IMPLEMENT IT MYSELF :MOYAI:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling toomuchformetohandle_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each toomuchformetohandle_column variable scan the preprocessedDataTrain variable (in context the datatype is pandas dataframe) for each column that is NOT Integer/float replace the value of the whole column with the count (from 0 to maximum unique value) of self-assigned unique integer for unique value\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Do embedding vectorization due to >100 unique thing however if its <=100 then do \n",
    "threshold_word2vec=100 # above threshold / >100 is too much cardinality use word2vec // set it to 9999999 to disable\n",
    "\n",
    "# Iterate through specified columns\n",
    "for column in toomuchformetohandle_column:\n",
    "    # Assuming your data is a list of strings\n",
    "    sentences = preprocessedDataTrain[column].tolist()\n",
    "    #print(sentences)\n",
    "    # Tokenize sentences (assuming whitespace tokenization)\n",
    "    tokenized_sentences = [sentence.split() for sentence in sentences]\n",
    "\n",
    "    # Train Word2Vec model\n",
    "    model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "    # Function to convert a sentence to a vector\n",
    "    def sentence_to_vector_mean(sentence):\n",
    "        vector = np.zeros((model.vector_size,))\n",
    "        count = 0\n",
    "        for word in sentence:\n",
    "            if word in model.wv:\n",
    "                vector += model.wv[word]\n",
    "                count += 1\n",
    "        result0 = vector / count if count > 0 else vector\n",
    "        #print(f\"result0 Vector Array {result0}\")\n",
    "        result1 = np.mean(result0)\n",
    "        #print(f\"result1 Vector Array Mean {result1}\")\n",
    "        return result1\n",
    "\n",
    "    # Convert each sentence to a vector\n",
    "    vectors = [sentence_to_vector_mean(sentence) for sentence in tokenized_sentences]\n",
    "\n",
    "    # Replace the column with the vectors\n",
    "    preprocessedDataTrain[column] = vectors\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final inspection make sure everything already being processed into data appropriately to be feed into the advanced \"Calculator\" which making sure its not Null or String and everything is number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessedDataTrain.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization of the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "standard_scaler = StandardScaler()\n",
    "min_max_scaler = MinMaxScaler()\n",
    "#pd.DataFrame(df_scaled_standard, columns=df_encoded.columns)\n",
    "preprocessedDataTrainTarget = preprocessedDataTrain[target_column]\n",
    "preprocessedDataTrain = pd.DataFrame(standard_scaler.fit_transform(preprocessedDataTrain), columns=preprocessedDataTrain.columns)\n",
    "#exclude target_column\n",
    "preprocessedDataTrain[target_column] = preprocessedDataTrainTarget\n",
    "#view\n",
    "preprocessedDataTrain.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#split the data 69% for training 31% for evaluation\n",
    "\n",
    "\n",
    "#Convert into target accuracy float training datatype\n",
    "\n",
    "datatypeProcessing = ＯＢＯＲ.float32 # I don't know what's the difference if this uses float16 or float32, but the default is float32 i presume the Nvidia default\n",
    "\n",
    "\n",
    "# Assuming your target variable is stored in 'target_column'\n",
    "X = preprocessedDataTrain.drop(columns=[target_column])\n",
    "y = preprocessedDataTrain[target_column]\n",
    "\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=0.31, random_state=69420) #lets split it 0.31 so we can get the dataset 0.69 or 69%, Nice.\n",
    "\n",
    "# Optionally, if you want to convert them back to DataFrames\n",
    "training_data = pd.concat([X_train, y_train], axis=1)\n",
    "evaluation_data = pd.concat([X_eval, y_eval], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The end of Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The start of NN Architecturing define"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Identify the target of your Data analysis need!\n",
    "\n",
    "1. **Feedforward Neural Network (FNN)**:\n",
    "   - This is the simplest form of neural network where connections between nodes do not form a cycle.\n",
    "   - Information moves in only one direction: forward, from the input nodes, through the hidden layers (if any), and finally to the output nodes.\n",
    "\n",
    "2. **Convolutional Neural Network (CNN)**:\n",
    "   - Primarily used for image classification and recognition tasks.\n",
    "   - CNNs use a variation of multilayer perceptrons designed to require minimal preprocessing.\n",
    "\n",
    "3. **Recurrent Neural Network (RNN)**:\n",
    "   - Designed to recognize patterns in sequences of data, such as time series or sentences in natural language processing tasks.\n",
    "   - RNNs use feedback loops to process sequential data, making them capable of handling inputs of arbitrary length.\n",
    "\n",
    "4. **Long Short-Term Memory (LSTM)**:\n",
    "   - A type of RNN architecture that is capable of learning long-term dependencies.\n",
    "   - LSTMs are well-suited for tasks where the gap between relevant information can be large, such as speech recognition or language modeling.\n",
    "\n",
    "I know there's more than this, but for now lets focus on these thing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Contemplate from the target and the points lets use RNN, since the property of this dataset is \"With these combination of circumtances, will his/her endeavour suceed?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have data from preprocessed training_data and evaluation_data\n",
    "# now we need to know the dimensions of the input \n",
    "\n",
    "\n",
    "#start of the required Model dimension automatic check\n",
    "\n",
    "def get_model_dimensions(training_data):\n",
    "    # Extract input size from the number of features (excluding the target column)\n",
    "    input_size = training_data.shape[1] - 1  # Assuming the target column is the last column\n",
    "\n",
    "    # Define hidden size based on a rule of thumb or domain knowledge\n",
    "    hidden_size = 32\n",
    "\n",
    "    # Define number of layers based on a rule of thumb or domain knowledge\n",
    "    num_layers = 24\n",
    "\n",
    "    # Extract output size from the number of unique classes in the target column\n",
    "    output_size = training_data[target_column].nunique()\n",
    "\n",
    "    return input_size, hidden_size, num_layers, output_size\n",
    "\n",
    "\n",
    "input_size, hidden_size, num_layers, output_size = get_model_dimensions(training_data)\n",
    "print(f\"NN Architecture Requirement! : Input size: {input_size}, Hidden size: {hidden_size}, Number of layers: {num_layers}, Output size: {output_size}\")\n",
    "#End of the required model dimensions automatic check\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "dynamicArchStubMode=True #disable dynamic Architecturing\n",
    "## Architecturing Start!\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Architecturing for Transformer (??)\n",
    "\n",
    "\n",
    "\n",
    "# Architecture for Time Scaling LSTM\n",
    "\n",
    "\n",
    "\n",
    "# Architecture for RNN Pure NLP Vectorization\n",
    "\n",
    "\n",
    "# Architecture for Non time scaling FNN only Situation\n",
    "class ModelArch(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(ModelArch, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Dense layers\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward propagate through Dense layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        # Softmax activation\n",
    "        out = F.softmax(x, dim=1)\n",
    "        return out\n",
    "# Example usage\n",
    "# Create an instance of the RNN model\n",
    "    # Example usage\n",
    "\n",
    "model = ModelArch(input_size, hidden_size, num_layers, output_size)\n",
    "\n",
    "\n",
    "# Print the model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initialize Acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if MPS APU is available, or rocm, CUDA GPU is available (wait what's mps and rocm anyway?), otherwise use CPU\n",
    "if ＯＢＯＲ.cuda.is_available():\n",
    "    device = ＯＢＯＲ.device('cuda')\n",
    "    print(\"Nvidiah exclusive!\")\n",
    "elif ＯＢＯＲ.backends.mps.is_available():\n",
    "    device = ＯＢＯＲ.device('mps')\n",
    "    print(\"Trying to break exclusivity eh?\")\n",
    "elif ＯＢＯＲ.backends.rocm.is_available():\n",
    "    device = ＯＢＯＲ.device('rocm')\n",
    "    print(\"you're a gigachad\")\n",
    "else:\n",
    "    device = ＯＢＯＲ.device('cpu')\n",
    "    print(\"too bad, your training speed will be slow\")\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before we get into further training step we're going to learn about loss algorithm, check what is applicable for your specific data\n",
    "Loss algorithm callbacks, also known as loss functions or objective functions, play a crucial role in training machine learning models. Here are some common types of loss functions used in various machine learning tasks:\n",
    "\n",
    "1. **Mean Squared Error (MSE)**:\n",
    "   - Used in regression tasks to measure the average squared difference between the predicted values and the actual values.\n",
    "\n",
    "2. **Mean Absolute Error (MAE)**:\n",
    "   - Similar to MSE but measures the average absolute difference between the predicted values and the actual values.\n",
    "\n",
    "3. **Binary Cross-Entropy Loss**:\n",
    "   - Used in binary classification tasks to measure the difference between probability distributions, typically when the target variable is binary.\n",
    "\n",
    "4. **Categorical Cross-Entropy Loss**:\n",
    "   - Used in multi-class classification tasks to measure the difference between probability distributions when the target variable is categorical.\n",
    "\n",
    "5. **Sparse Categorical Cross-Entropy Loss**:\n",
    "   - Similar to categorical cross-entropy loss but used when the target variable is represented as integers instead of one-hot encoded vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lets do automation\n",
    "#### TODO: create a code where it check for the target_column variable unique sum if its 2 then use \"binary Cross-entropy Loss\" but if its more than 2 then \"categorical cross-entropy loss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss functions\n",
    "criterion_mse = nn.MSELoss()\n",
    "criterion_bce = nn.BCEWithLogitsLoss()\n",
    "criterion_ce = nn.CrossEntropyLoss()\n",
    "\n",
    "#check target uniqueness\n",
    "# uniqueSum = training_data[target_column].nunique()\n",
    "uniqueSum = training_data[target_column].nunique()\n",
    "if uniqueSum < 2:\n",
    "    print(\"Invalid Data!\")\n",
    "    exit\n",
    "elif uniqueSum == 2:\n",
    "    criterion = criterion_bce\n",
    "elif uniqueSum > 2:\n",
    "    criterion = criterion_mse\n",
    "\n",
    "criterion = criterion_ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperOS parameter, jk. Hyperparameter\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "import torch.optim as optim\n",
    "#define optimizer. You don't need to learn other optimizer, because others sucks, Adam ftw!\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# define tensor training\n",
    "    \n",
    "# Convert training and evaluation data to PyTorch tensors\n",
    "\n",
    "\n",
    "#main target\n",
    "X_train_tensor = ＯＢＯＲ.tensor(training_data.drop(columns=[target_column]).values, dtype=datatypeProcessing)\n",
    "#X_train_tensor = X_train_tensor.unsqueeze(1)\n",
    "y_train_tensor = ＯＢＯＲ.tensor(training_data[target_column].values, dtype=datatypeProcessing)\n",
    "#y_train_tensor = y_train_tensor.unsqueeze(1)\n",
    "# main evaluation\n",
    "X_eval_tensor = ＯＢＯＲ.tensor(evaluation_data.drop(columns=[target_column]).values, dtype=datatypeProcessing)\n",
    "X_eval_tensor = X_eval_tensor.unsqueeze(1)\n",
    "y_eval_tensor = ＯＢＯＲ.tensor(evaluation_data[target_column].values, dtype=datatypeProcessing)\n",
    "#y_eval_tensor = y_eval_tensor.unsqueeze(1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the training data to DataLoader for batching\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_dataset = ＯＢＯＲ.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = ＯＢＯＲ.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "train_loss_history = []\n",
    "train_accuracy_history = []\n",
    "val_loss_history = []\n",
    "val_accuracy_history = []\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "num_folds = 5  # You can adjust the number of folds as needed\n",
    "kf = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "fold = 0\n",
    "thresholdLoss=0.1 #cut training if its drop below this threshold\n",
    "thresholdLostEnable=True\n",
    "for train_index, val_index in kf.split(X_train_tensor):\n",
    "    fold += 1\n",
    "    print(f'Fold {fold}/{num_folds}')\n",
    "    \n",
    "    # Extract training and validation data for this fold\n",
    "    X_fold_train, X_fold_val = X_train_tensor[train_index], X_train_tensor[val_index]\n",
    "    y_fold_train, y_fold_val = y_train_tensor[train_index], y_train_tensor[val_index]\n",
    "    \n",
    "    # Convert training data to DataLoader for batching\n",
    "    fold_train_dataset = ＯＢＯＲ.utils.data.TensorDataset(X_fold_train, y_fold_train)\n",
    "    fold_train_loader = ＯＢＯＲ.utils.data.DataLoader(dataset=fold_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training the model\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_X, batch_y in fold_train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            _, predicted = ＯＢＯＲ.max(outputs, 1)\n",
    "            correct += (predicted == batch_y.to(device)).sum().item()\n",
    "            #print(f\"NN Verbose outputs Model Total {outputs} \\n NN Verbose outputs y batch tensor {batch_y} \\n NN Verbose _ ? {_} \\n Activated NN Decoded {predicted}\")\n",
    "            #print(f\"varDebug Param {total_loss} {correct} {total}\")\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward() #Backpropagation\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            total += batch_y.size(0)\n",
    "\n",
    "        if (loss <= thresholdLoss and thresholdLostEnable):\n",
    "            print(\"Preventing Overfitting!\")\n",
    "            break\n",
    "        # Print average loss and accuracy for each epoch\n",
    "        accuracy = 100 * correct / total\n",
    "        loss = total_loss / len(fold_train_loader)\n",
    "        if ( epoch % 50 == 0 ):\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Fold [{fold}/{num_folds}], Loss: {loss}, Accuracy: {accuracy:.2f}%')\n",
    "        train_accuracy_history.append(accuracy)\n",
    "        train_loss_history.append(loss)\n",
    "\n",
    "        # Validation for this Epoch\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        with ＯＢＯＲ.no_grad():\n",
    "            outputs = model(X_fold_val.to(device))\n",
    "            val_loss = criterion(outputs, y_fold_val.to(device))\n",
    "            _, predicted = ＯＢＯＲ.max(outputs, 1)\n",
    "            val_correct = (predicted == y_fold_val.to(device)).sum().item()\n",
    "            val_accuracy = 100 * val_correct / len(y_fold_val)\n",
    "            if ( epoch % 50 == 0 ):\n",
    "                print(f'Fold [{fold}/{num_folds}], Validation Loss: {val_loss.item():.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n",
    "            val_accuracy_history.append(accuracy)\n",
    "            val_loss_history.append(loss)\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "# Save the trained model\n",
    "ＯＢＯＲ.save(model.state_dict(), 'ＯＢＯＲ_MODEL.pth')\n",
    "\n",
    "print('Training finished.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shift the plot to increase visibility\n",
    "plotshift=2\n",
    "\n",
    "plt.plot(train_loss_history)\n",
    "plt.plot(range(plotshift, len(val_loss_history) + plotshift), val_loss_history, linestyle='--')  # Shifting validation loss by 4 epochs\n",
    "plt.title(\"Model loss\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"Train\", \"Validation\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shift the plot to increase visibility\n",
    "plotshift=2\n",
    "\n",
    "plt.plot(train_accuracy_history)\n",
    "plt.plot(range(plotshift, len(val_accuracy_history) + plotshift), val_accuracy_history, linestyle='--')  # Shifting validation loss by 4 epochs\n",
    "#plt.plot(train.history[\"val_accuracy\"])\n",
    "plt.title(\"Model Accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"Train\", \"Validation\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "\n",
    "#### I don't understand this one, yet. but i understand the concept by comparing with the split data of the eval 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the evaluation data to DataLoader for processing the entire dataset at once\n",
    "eval_dataset = ＯＢＯＲ.utils.data.TensorDataset(X_eval_tensor, y_eval_tensor)\n",
    "eval_loader = ＯＢＯＲ.utils.data.DataLoader(dataset=eval_dataset, batch_size=len(eval_dataset))\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "total_eval_loss = 0\n",
    "with ＯＢＯＲ.no_grad():  # Disable gradient calculation\n",
    "    print(eval_loader)\n",
    "    for eval_X, eval_y in eval_loader:\n",
    "        print(\"spamdebug\")\n",
    "        # Move evaluation tensors to the selected device\n",
    "        eval_X, eval_y = eval_X.to(device), eval_y.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        eval_outputs = model(eval_X)\n",
    "        eval_loss = criterion(eval_outputs, eval_y)\n",
    "\n",
    "        total_eval_loss += eval_loss.item()\n",
    "\n",
    "# Calculate average evaluation loss\n",
    "average_eval_loss = total_eval_loss / len(eval_loader)\n",
    "\n",
    "# You can add other evaluation metrics here such as accuracy, precision, recall, etc.\n",
    "\n",
    "# Print evaluation results\n",
    "print(f'Evaluation Loss: {average_eval_loss:.4f}')\n",
    "\n",
    "# Additional evaluation metrics can be printed here as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
